{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st033822/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/st033822/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f87cfa5e730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sklearn\n",
    "import socket\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return \"{}proxy/{}/jobs/\".format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "# small fix to enable UI views\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# spark configurtion in local regime \n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '8g')\n",
    "\n",
    "#some needed objects\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "Transform text file \"The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelle\" into TF-IDF. Take sentence as \"document\".\n",
    "\n",
    "### Part 1: \n",
    "- read text file as dataframe \n",
    "- filter out non-letters and empty strings \n",
    "- transform into dataframe doc_id -> tf_idf vector \n",
    "\n",
    "\n",
    "### Part 2:\n",
    "- read text file as RDD\n",
    "- filter out non-letters and empty strings \n",
    "- transform into rdd in format doc_id -> tf_idf vector\n",
    "\n",
    "\n",
    "### NB(!) \n",
    "\n",
    "It's not allowed to use TF-IDF code from Spark internal libraries. \n",
    "It's not allowed to cast DF/RDD into pandas and use scikit-learn. Please, keep it spark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Reading text file as rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                text| id|\n",
      "+--------------------+---+\n",
      "|The Project Guten...|  0|\n",
      "|                    |  1|\n",
      "|This eBook is for...|  2|\n",
      "|most other parts ...|  3|\n",
      "|whatsoever. You m...|  4|\n",
      "|of the Project Gu...|  5|\n",
      "|www.gutenberg.org...|  6|\n",
      "|will have to chec...|  7|\n",
      "|   using this eBook.|  8|\n",
      "|                    |  9|\n",
      "| Title: Frankenstein| 10|\n",
      "|       or, The Mo...| 11|\n",
      "|                    | 12|\n",
      "|Author: Mary Woll...| 13|\n",
      "|                    | 14|\n",
      "|Release Date: 31,...| 15|\n",
      "|[Most recently up...| 16|\n",
      "|                    | 17|\n",
      "|   Language: English| 18|\n",
      "|                    | 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_prefix = \"malyutin_demo_hw1\"\n",
    "\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dataframe = sc.textFile(f\"{filepath}\")\\\n",
    "    .map(lambda x: (x,))\\\n",
    "    .toDF()\\\n",
    "    .select(F.col(\"_1\").alias(\"text\"))\\\n",
    "    .withColumn(\"id\", monotonically_increasing_id())\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def process_string(data):\n",
    "    \"\"\"\n",
    "    basic preprocessing function:\n",
    "    - removes punctuation\n",
    "    - lower\n",
    "    - split by space\n",
    "    \"\"\"\n",
    "    punct_removed = re.sub(r'[^\\w\\s]','',data)\n",
    "    words = punct_removed.lower().split(\" \")\n",
    "    \n",
    "    \n",
    "    return list(filter(lambda x: len(x) > 0, words))\n",
    "\n",
    "# spark udf -- user defined function (~ mapper)\n",
    "\n",
    "process_string_udf = udf(lambda z: process_string(z), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            by_words|doc_id|\n",
      "+--------------------+------+\n",
      "|[the, project, gu...|     0|\n",
      "|[this, ebook, is,...|     1|\n",
      "|[most, other, par...|     2|\n",
      "|[whatsoever, you,...|     3|\n",
      "|[of, the, project...|     4|\n",
      "|[wwwgutenbergorg,...|     5|\n",
      "|[will, have, to, ...|     6|\n",
      "|[using, this, ebook]|     7|\n",
      "|[title, frankenst...|     8|\n",
      "|[or, the, modern,...|     9|\n",
      "|[author, mary, wo...|    10|\n",
      "|[release, date, 3...|    11|\n",
      "|[most, recently, ...|    12|\n",
      "| [language, english]|    13|\n",
      "|[character, set, ...|    14|\n",
      "|[produced, by, ju...|    15|\n",
      "|[further, correct...|    16|\n",
      "|[start, of, the, ...|    17|\n",
      "|[or, the, modern,...|    18|\n",
      "|[by, mary, wollst...|    19|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "process words\n",
    "\n",
    "filter out empty and small sentences\n",
    "\"\"\"\n",
    "\n",
    "by_words = dataframe\\\n",
    "    .select(process_string_udf(F.col(\"text\")).alias(\"by_words\"))\\\n",
    "    .where(F.size(F.col(\"by_words\")) > 1)\\\n",
    "    .withColumn(\"doc_id\", monotonically_increasing_id())\n",
    "by_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|          word|doc_id|\n",
      "+--------------+------+\n",
      "|           the|     0|\n",
      "|       project|     0|\n",
      "|     gutenberg|     0|\n",
      "|         ebook|     0|\n",
      "|            of|     0|\n",
      "|  frankenstein|     0|\n",
      "|            by|     0|\n",
      "|          mary|     0|\n",
      "|wollstonecraft|     0|\n",
      "|        godwin|     0|\n",
      "|       shelley|     0|\n",
      "|          this|     1|\n",
      "|         ebook|     1|\n",
      "|            is|     1|\n",
      "|           for|     1|\n",
      "|           the|     1|\n",
      "|           use|     1|\n",
      "|            of|     1|\n",
      "|        anyone|     1|\n",
      "|      anywhere|     1|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exploding to get separate words\n",
    "by_words_count = by_words.select(F.explode(F.col(\"by_words\")).alias(\"word\"), F.col(\"doc_id\"))\n",
    "by_words_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+\n",
      "|doc_id|      word| tf|\n",
      "+------+----------+---+\n",
      "|    12|        13|  1|\n",
      "|    62|      vain|  1|\n",
      "|    92|expedition|  1|\n",
      "|   127| fluctuate|  1|\n",
      "|   170|   approve|  1|\n",
      "|   171|      poor|  1|\n",
      "|   189|        my|  1|\n",
      "|   259|  remember|  1|\n",
      "|   315|        in|  2|\n",
      "|   319|     until|  1|\n",
      "|   339|         i|  1|\n",
      "|   419|  although|  1|\n",
      "|   453|         a|  1|\n",
      "|   477|        to|  1|\n",
      "|   480|       you|  1|\n",
      "|   506|        in|  1|\n",
      "|   513|      what|  1|\n",
      "|   515|  greatest|  1|\n",
      "|   515|       but|  1|\n",
      "|   536|  beaufort|  1|\n",
      "+------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating term frequency\n",
    "\n",
    "by_words_count = by_words_count.groupBy(\"doc_id\", \"word\")\\\n",
    "    .agg(F.count(\"word\").alias(\"tf\"))\n",
    "by_words_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|         word| df|\n",
      "+-------------+---+\n",
      "|         some|145|\n",
      "|         hope| 49|\n",
      "|        those| 91|\n",
      "|          art|  7|\n",
      "|    solemnity|  4|\n",
      "|      blossom|  1|\n",
      "|          few| 62|\n",
      "|     ignominy|  4|\n",
      "|        still| 65|\n",
      "|       voyage| 15|\n",
      "|    imitation|  2|\n",
      "|       waters| 11|\n",
      "|gratification|  3|\n",
      "|    arguments|  7|\n",
      "|      barrier|  4|\n",
      "|   circulates|  1|\n",
      "|apprehensions|  2|\n",
      "|       spared|  5|\n",
      "|     painters|  1|\n",
      "|  transaction|  1|\n",
      "+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document frequency\n",
    "\n",
    "by_words_count_with_df = by_words_count.groupBy(\"word\")\\\n",
    "      .agg(F.countDistinct(\"doc_id\").alias(\"df\"))\n",
    "by_words_count_with_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the same formula to calculate IDF that is included in Spark MLlib\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import log, log1p\n",
    "number_of_documents = dataframe.count()\n",
    "import numpy as np\n",
    "def calc_idf(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: document frequency - how many times a given word appears in whole corpus of docs (without normalization)\n",
    "        \n",
    "    Returns:\n",
    "        idf: inverse document frequency\n",
    "    \"\"\"\n",
    "    exp_idf = (number_of_documents + 1)/(df + 1) \n",
    "    idf = np.log(exp_idf)\n",
    "    return idf\n",
    "\n",
    "calc_idf_udf = udf(lambda df: calc_idf(docCount, df), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import log, log1p\n",
    "number_of_documents = dataframe.count()\n",
    "import numpy as np\n",
    "def calc_idf_exp(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: document frequency - how many times a given word appears in whole corpus of docs (without normalization)\n",
    "        \n",
    "    Returns:\n",
    "        exp_idf: inverse document frequency without taking log\n",
    "    \"\"\"\n",
    "    exp_idf = (number_of_documents + 1)/(df + 1) \n",
    "\n",
    "    return exp_idf\n",
    "\n",
    "calc_idf_exp_udf = udf(lambda df: calc_idf(docCount, df), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+------------------+\n",
      "|         word| df|               idf|\n",
      "+-------------+---+------------------+\n",
      "|         some|145|3.9710670072480765|\n",
      "|         hope| 49| 5.042650623528266|\n",
      "|        those| 91| 4.432885051907372|\n",
      "|          art|  7| 6.875232087276577|\n",
      "|    solemnity|  4| 7.345235716522312|\n",
      "|      blossom|  1| 8.261526448396468|\n",
      "|          few| 62|  4.81153890256488|\n",
      "|     ignominy|  4| 7.345235716522312|\n",
      "|        still| 65| 4.765018886929988|\n",
      "|       voyage| 15| 6.182084906716632|\n",
      "|    imitation|  2| 7.856061340288304|\n",
      "|       waters| 11| 6.469766979168413|\n",
      "|gratification|  3| 7.568379267836522|\n",
      "|    arguments|  7| 6.875232087276577|\n",
      "|      barrier|  4| 7.345235716522312|\n",
      "|   circulates|  1| 8.261526448396468|\n",
      "|apprehensions|  2| 7.856061340288304|\n",
      "|       spared|  5| 7.162914159728358|\n",
      "|     painters|  1| 8.261526448396468|\n",
      "|  transaction|  1| 8.261526448396468|\n",
      "+-------------+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_with_idf = by_words_count_with_df.withColumn(\"idf\", calc_idf(F.col(\"df\")))\n",
    "by_words_with_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+-----------------+----------+---+-----------------+\n",
      "|         word| df|              idf|    doc_id| tf|           tf_idf|\n",
      "+-------------+---+-----------------+----------+---+-----------------+\n",
      "| accumulation|  2|7.856061340288304|      2614|  1|7.856061340288304|\n",
      "| accumulation|  2|7.856061340288304|8589937145|  1|7.856061340288304|\n",
      "|apprehensions|  2|7.856061340288304|       785|  1|7.856061340288304|\n",
      "|apprehensions|  2|7.856061340288304|      1511|  1|7.856061340288304|\n",
      "|    arguments|  7|6.875232087276577|8589935320|  1|6.875232087276577|\n",
      "|    arguments|  7|6.875232087276577|      1734|  1|6.875232087276577|\n",
      "|    arguments|  7|6.875232087276577|8589936445|  1|6.875232087276577|\n",
      "|    arguments|  7|6.875232087276577|      2676|  1|6.875232087276577|\n",
      "|    arguments|  7|6.875232087276577|      2327|  1|6.875232087276577|\n",
      "|    arguments|  7|6.875232087276577|       870|  1|6.875232087276577|\n",
      "|    arguments|  7|6.875232087276577|       423|  1|6.875232087276577|\n",
      "|          art|  7|6.875232087276577|      3093|  1|6.875232087276577|\n",
      "|          art|  7|6.875232087276577|8589936551|  1|6.875232087276577|\n",
      "|          art|  7|6.875232087276577|      2605|  1|6.875232087276577|\n",
      "|          art|  7|6.875232087276577|      2599|  1|6.875232087276577|\n",
      "|          art|  7|6.875232087276577|       416|  1|6.875232087276577|\n",
      "|          art|  7|6.875232087276577|      1402|  1|6.875232087276577|\n",
      "|          art|  7|6.875232087276577|8589936925|  1|6.875232087276577|\n",
      "|      barrier|  4|7.345235716522312|8589935638|  1|7.345235716522312|\n",
      "|      barrier|  4|7.345235716522312|8589935637|  1|7.345235716522312|\n",
      "+-------------+---+-----------------+----------+---+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_full = by_words_with_idf.join(by_words_count, on=[\"word\"], how = \"left_outer\")\\\n",
    "    .withColumn(\"tf_idf\", F.col(\"tf\") * F.col(\"idf\"))\n",
    "tf_idf_full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|doc_id|                                                                                              tf_idf|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|     0|[2.7788063588506517, 7.568379267836522, 7.568379267836522, 4.466037259224273, 7.568379267836522, ...|\n",
      "|     1|[6.652088535962367, 2.7281369596689475, 1.9273591149166367, 5.91015119123299, 3.231088527004032, ...|\n",
      "|     2|[7.162914159728358, 2.410761783391621, 4.432885051907372, 7.568379267836522, 0.8581607114548204, ...|\n",
      "|     3|[6.469766979168413, 2.644755350729896, 5.488937726156687, 5.553476247294258, 4.263325746727269, 7...|\n",
      "|     4|[7.345235716522312, 2.410761783391621, 4.466037259224273, 0.8581607114548204, 7.345235716522312, ...|\n",
      "|     5|[2.70084481738094, 5.289510701459792, 1.9273591149166367, 5.91015119123299, 6.875232087276577, 5....|\n",
      "|     6|[3.7289269552432116, 2.644755350729896, 6.875232087276577, 6.010234649789973, 7.162914159728358, ...|\n",
      "|     7|                                           [6.875232087276577, 6.315616299341154, 2.861103858911277]|\n",
      "|     8|                                                              [7.856061340288304, 5.587377798969939]|\n",
      "|     9|                     [0.8581607114548204, 7.856061340288304, 6.3897242714948765, 3.4616121856158646]|\n",
      "|    10|     [7.568379267836522, 7.008763479901099, 7.568379267836522, 7.568379267836522, 7.568379267836522]|\n",
      "|    11|[8.261526448396468, 7.008763479901099, 8.261526448396468, 8.261526448396468, 6.315616299341154, 7...|\n",
      "|    12|[8.261526448396468, 8.261526448396468, 7.856061340288304, 7.856061340288304, 4.400796737355872, 7...|\n",
      "|    13|                                                               [5.91015119123299, 6.652088535962367]|\n",
      "|    14|                        [6.757449051620194, 5.863631175598097, 8.261526448396468, 8.261526448396468]|\n",
      "|    15|[8.261526448396468, 5.5576127177013035, 8.261526448396468, 8.261526448396468, 6.246623427854203, ...|\n",
      "|    16|[2.7788063588506517, 8.261526448396468, 6.469766979168413, 7.568379267836522, 8.261526448396468, ...|\n",
      "|    17|[4.466037259224273, 0.8581607114548204, 1.1565610001266253, 6.315616299341154, 5.488937726156687,...|\n",
      "|    18|                     [0.8581607114548204, 7.856061340288304, 6.3897242714948765, 3.4616121856158646]|\n",
      "|    19|    [2.7788063588506517, 7.568379267836522, 7.568379267836522, 7.568379267836522, 7.568379267836522]|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_aggregated = tf_idf_full.groupBy('doc_id')\\\n",
    "    .agg(F.collect_list('tf_idf').alias('tf_idf'))\\\n",
    "    .orderBy(F.col(\"doc_id\"))\n",
    "tf_idf_aggregated.show(truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|doc_id|                                                                                              tf_idf|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|     0|[2.7788063588506517, 7.568379267836522, 7.568379267836522, 4.466037259224273, 7.568379267836522, ...|\n",
      "|     1|[6.652088535962367, 2.7281369596689475, 1.9273591149166367, 5.91015119123299, 3.231088527004032, ...|\n",
      "|     2|[7.162914159728358, 2.410761783391621, 4.432885051907372, 7.568379267836522, 0.8581607114548204, ...|\n",
      "|     3|[6.469766979168413, 2.644755350729896, 5.488937726156687, 5.553476247294258, 4.263325746727269, 7...|\n",
      "|     4|[7.345235716522312, 2.410761783391621, 4.466037259224273, 0.8581607114548204, 7.345235716522312, ...|\n",
      "|     5|[2.70084481738094, 5.289510701459792, 1.9273591149166367, 5.91015119123299, 6.875232087276577, 5....|\n",
      "|     6|[3.7289269552432116, 2.644755350729896, 6.875232087276577, 6.010234649789973, 7.162914159728358, ...|\n",
      "|     7|                                           [6.875232087276577, 6.315616299341154, 2.861103858911277]|\n",
      "|     8|                                                              [7.856061340288304, 5.587377798969939]|\n",
      "|     9|                     [0.8581607114548204, 7.856061340288304, 6.3897242714948765, 3.4616121856158646]|\n",
      "|    10|     [7.568379267836522, 7.008763479901099, 7.568379267836522, 7.568379267836522, 7.568379267836522]|\n",
      "|    11|[8.261526448396468, 7.008763479901099, 8.261526448396468, 8.261526448396468, 6.315616299341154, 7...|\n",
      "|    12|[8.261526448396468, 8.261526448396468, 7.856061340288304, 7.856061340288304, 4.400796737355872, 7...|\n",
      "|    13|                                                               [5.91015119123299, 6.652088535962367]|\n",
      "|    14|                        [6.757449051620194, 5.863631175598097, 8.261526448396468, 8.261526448396468]|\n",
      "|    15|[8.261526448396468, 5.5576127177013035, 8.261526448396468, 8.261526448396468, 6.246623427854203, ...|\n",
      "|    16|[2.7788063588506517, 8.261526448396468, 6.469766979168413, 7.568379267836522, 8.261526448396468, ...|\n",
      "|    17|[4.466037259224273, 0.8581607114548204, 1.1565610001266253, 6.315616299341154, 5.488937726156687,...|\n",
      "|    18|                     [0.8581607114548204, 7.856061340288304, 6.3897242714948765, 3.4616121856158646]|\n",
      "|    19|    [2.7788063588506517, 7.568379267836522, 7.568379267836522, 7.568379267836522, 7.568379267836522]|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# whole pipeline\n",
    "by_words_count = sc.textFile(f\"{filepath}\")\\\n",
    "    .map(lambda x: (x,))\\\n",
    "    .toDF()\\\n",
    "    .select(F.col(\"_1\").alias(\"text\"))\\\n",
    "    .select(process_string_udf(F.col(\"text\")).alias(\"by_words\"))\\\n",
    "    .where(F.size(F.col(\"by_words\")) > 1)\\\n",
    "    .withColumn(\"doc_id\", monotonically_increasing_id())\\\n",
    "    .select(F.explode(F.col(\"by_words\")).alias(\"word\"), F.col(\"doc_id\"))\\\n",
    "    .groupBy(\"doc_id\", \"word\")\\\n",
    "    .agg(F.count(\"word\").alias(\"tf\"))\n",
    "\n",
    "   \n",
    "\n",
    "by_words_with_idf = by_words_count.groupBy(\"word\")\\\n",
    "    .agg(F.countDistinct(\"doc_id\").alias(\"df\"))\\\n",
    "    .withColumn(\"idf\", log(calc_idf_exp(F.col(\"df\"))))\n",
    "    \n",
    "\n",
    "tf_idf = by_words_with_idf.join(by_words_count, on=[\"word\"], how = \"left_outer\")\\\n",
    "    .withColumn(\"tf_idf\", F.col(\"tf\") * F.col(\"idf\"))\\\n",
    "    .groupBy('doc_id')\\\n",
    "    .agg(F.collect_list('tf_idf').alias('tf_idf'))\\\n",
    "    .orderBy(F.col(\"doc_id\"))\n",
    "\n",
    "tf_idf.show(truncate = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Reading text file into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Letter 2', 40),\n",
       " (' Letter 3', 41),\n",
       " (' Letter 4', 42),\n",
       " (' Chapter 1', 43),\n",
       " (' Chapter 2', 44),\n",
       " (' Chapter 3', 45),\n",
       " (' Chapter 4', 46),\n",
       " (' Chapter 5', 47),\n",
       " (' Chapter 6', 48),\n",
       " (' Chapter 7', 49),\n",
       " ('Inspirited by this wind of promise, my daydreams become more fervent', 90),\n",
       " ('and vivid. I try in vain to be persuaded that the pole is the seat of', 91),\n",
       " ('frost and desolation; it ever presents itself to my imagination as the',\n",
       "  92),\n",
       " ('region of beauty and delight. There, Margaret, the sun is for ever', 93),\n",
       " ('visible, its broad disk just skirting the horizon and diffusing a', 94),\n",
       " ('perpetual splendour. There—for with your leave, my sister, I will put', 95),\n",
       " ('some trust in preceding navigators—there snow and frost are banished;', 96),\n",
       " ('and, sailing over a calm sea, we may be wafted to a land surpassing in',\n",
       "  97),\n",
       " ('wonders and in beauty every region hitherto discovered on the habitable',\n",
       "  98),\n",
       " ('globe. Its productions and features may be without example, as the', 99)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_prefix = \"malyutin_demo_hw1\"\n",
    "\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "rddText = sc.textFile(f\"{filepath}\").repartition(1).zipWithIndex().repartition(5)\n",
    "\n",
    "\n",
    "\n",
    "rddText.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['letter', '2'],\n",
       " ['letter', '3'],\n",
       " ['letter', '4'],\n",
       " ['chapter', '1'],\n",
       " ['chapter', '2'],\n",
       " ['chapter', '3'],\n",
       " ['chapter', '4'],\n",
       " ['chapter', '5'],\n",
       " ['chapter', '6'],\n",
       " ['chapter', '7'],\n",
       " ['inspirited',\n",
       "  'by',\n",
       "  'this',\n",
       "  'wind',\n",
       "  'of',\n",
       "  'promise',\n",
       "  'my',\n",
       "  'daydreams',\n",
       "  'become',\n",
       "  'more',\n",
       "  'fervent'],\n",
       " ['and',\n",
       "  'vivid',\n",
       "  'i',\n",
       "  'try',\n",
       "  'in',\n",
       "  'vain',\n",
       "  'to',\n",
       "  'be',\n",
       "  'persuaded',\n",
       "  'that',\n",
       "  'the',\n",
       "  'pole',\n",
       "  'is',\n",
       "  'the',\n",
       "  'seat',\n",
       "  'of'],\n",
       " ['frost',\n",
       "  'and',\n",
       "  'desolation',\n",
       "  'it',\n",
       "  'ever',\n",
       "  'presents',\n",
       "  'itself',\n",
       "  'to',\n",
       "  'my',\n",
       "  'imagination',\n",
       "  'as',\n",
       "  'the'],\n",
       " ['region',\n",
       "  'of',\n",
       "  'beauty',\n",
       "  'and',\n",
       "  'delight',\n",
       "  'there',\n",
       "  'margaret',\n",
       "  'the',\n",
       "  'sun',\n",
       "  'is',\n",
       "  'for',\n",
       "  'ever'],\n",
       " ['visible',\n",
       "  'its',\n",
       "  'broad',\n",
       "  'disk',\n",
       "  'just',\n",
       "  'skirting',\n",
       "  'the',\n",
       "  'horizon',\n",
       "  'and',\n",
       "  'diffusing',\n",
       "  'a']]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize RDD\n",
    "rdd_tokenized = rddText.map(lambda line: process_string(line[0]))\n",
    "rdd_tokenized.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['letter', '2'], 0),\n",
       " (['letter', '3'], 1),\n",
       " (['letter', '4'], 2),\n",
       " (['chapter', '1'], 3),\n",
       " (['chapter', '2'], 4),\n",
       " (['chapter', '3'], 5),\n",
       " (['chapter', '4'], 6),\n",
       " (['chapter', '5'], 7),\n",
       " (['chapter', '6'], 8),\n",
       " (['chapter', '7'], 9),\n",
       " (['inspirited',\n",
       "   'by',\n",
       "   'this',\n",
       "   'wind',\n",
       "   'of',\n",
       "   'promise',\n",
       "   'my',\n",
       "   'daydreams',\n",
       "   'become',\n",
       "   'more',\n",
       "   'fervent'],\n",
       "  10),\n",
       " (['and',\n",
       "   'vivid',\n",
       "   'i',\n",
       "   'try',\n",
       "   'in',\n",
       "   'vain',\n",
       "   'to',\n",
       "   'be',\n",
       "   'persuaded',\n",
       "   'that',\n",
       "   'the',\n",
       "   'pole',\n",
       "   'is',\n",
       "   'the',\n",
       "   'seat',\n",
       "   'of'],\n",
       "  11),\n",
       " (['frost',\n",
       "   'and',\n",
       "   'desolation',\n",
       "   'it',\n",
       "   'ever',\n",
       "   'presents',\n",
       "   'itself',\n",
       "   'to',\n",
       "   'my',\n",
       "   'imagination',\n",
       "   'as',\n",
       "   'the'],\n",
       "  12),\n",
       " (['region',\n",
       "   'of',\n",
       "   'beauty',\n",
       "   'and',\n",
       "   'delight',\n",
       "   'there',\n",
       "   'margaret',\n",
       "   'the',\n",
       "   'sun',\n",
       "   'is',\n",
       "   'for',\n",
       "   'ever'],\n",
       "  13),\n",
       " (['visible',\n",
       "   'its',\n",
       "   'broad',\n",
       "   'disk',\n",
       "   'just',\n",
       "   'skirting',\n",
       "   'the',\n",
       "   'horizon',\n",
       "   'and',\n",
       "   'diffusing',\n",
       "   'a'],\n",
       "  14)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rid of documents with length < 1 + indexing RDD\n",
    "rdd_tokenized = rdd_tokenized.filter(lambda x: len(x) > 1).zipWithIndex()\n",
    "rdd_tokenized.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('presents', 3),\n",
       " ('is', 305),\n",
       " ('perpetual', 5),\n",
       " ('put', 15),\n",
       " ('on', 461),\n",
       " ('undertaking', 13),\n",
       " ('to', 1896),\n",
       " ('branches', 12),\n",
       " ('alarmed', 6),\n",
       " ('at', 318),\n",
       " ('would', 179),\n",
       " ('one', 197),\n",
       " ('us', 67),\n",
       " ('distant', 10),\n",
       " ('of', 2435)]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document frequency - how many documents in the corpus contain the term\n",
    "\n",
    "rdd_df = rdd_tokenized\\\n",
    "    .map(lambda line: (line[1], line[0]))\\\n",
    "    .flatMapValues(lambda line: line)\\\n",
    "    .map(lambda line: (line[1], line[0]))\\\n",
    "    .distinct()\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(len)\n",
    "rdd_df.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('presents', 7.568379267836522),\n",
       " ('is', 3.231088527004032),\n",
       " ('perpetual', 7.162914159728358),\n",
       " ('put', 6.182084906716632),\n",
       " ('on', 2.819108737874674),\n",
       " ('undertaking', 6.315616299341154),\n",
       " ('to', 1.4066446590213988),\n",
       " ('branches', 6.3897242714948765),\n",
       " ('alarmed', 7.008763479901099),\n",
       " ('at', 3.1894825261715685),\n",
       " ('would', 3.761716778066203),\n",
       " ('one', 3.6664065982618776),\n",
       " ('us', 4.735165923780306),\n",
       " ('distant', 6.556778356158042),\n",
       " ('of', 1.1565610001266253)]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse document frequency\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import log1p\n",
    "rdd_idf = rdd_df\\\n",
    "    .map(lambda line: (line[0], calc_idf(line[1])))\n",
    "rdd_idf.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 'letter'), 1),\n",
       " ((3, 'chapter'), 1),\n",
       " ((5, '3'), 1),\n",
       " ((6, 'chapter'), 1),\n",
       " ((7, 'chapter'), 1),\n",
       " ((7, '5'), 1),\n",
       " ((9, '7'), 1),\n",
       " ((10, 'of'), 1),\n",
       " ((11, 'vain'), 1),\n",
       " ((12, 'imagination'), 1),\n",
       " ((13, 'there'), 1),\n",
       " ((13, 'the'), 1),\n",
       " ((13, 'for'), 1),\n",
       " ((13, 'ever'), 1),\n",
       " ((14, 'its'), 1)]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TERM FREQUENCY (tf)\n",
    "\n",
    "rdd_tf = rdd_tokenized\\\n",
    "    .map(lambda line: (line[1], line[0]))\\\n",
    "    .flatMapValues(lambda line: line)\\\n",
    "    .groupBy(lambda line: line)\\\n",
    "    .mapValues(len)\n",
    "rdd_tf.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing RDD with calculated TF for joining\n",
    "\n",
    "def mapper(rdd):\n",
    "    return ( rdd[0][1], (rdd[0][0], rdd[1]))\n",
    "\n",
    "rdd_tf_flat = rdd_tf.map(lambda x : mapper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('letter', (0, 1)),\n",
       " ('chapter', (3, 1)),\n",
       " ('3', (5, 1)),\n",
       " ('chapter', (6, 1)),\n",
       " ('chapter', (7, 1)),\n",
       " ('5', (7, 1)),\n",
       " ('7', (9, 1)),\n",
       " ('of', (10, 1)),\n",
       " ('vain', (11, 1)),\n",
       " ('imagination', (12, 1)),\n",
       " ('there', (13, 1)),\n",
       " ('the', (13, 1)),\n",
       " ('for', (13, 1)),\n",
       " ('ever', (13, 1)),\n",
       " ('its', (14, 1))]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_tf_flat.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', ((10, 1), 1.1565610001266253)),\n",
       " ('of', ((25, 1), 1.1565610001266253)),\n",
       " ('of', ((33, 1), 1.1565610001266253)),\n",
       " ('of', ((45, 1), 1.1565610001266253)),\n",
       " ('of', ((49, 1), 1.1565610001266253)),\n",
       " ('of', ((53, 1), 1.1565610001266253)),\n",
       " ('of', ((73, 1), 1.1565610001266253)),\n",
       " ('of', ((77, 1), 1.1565610001266253)),\n",
       " ('of', ((93, 1), 1.1565610001266253)),\n",
       " ('of', ((134, 1), 1.1565610001266253)),\n",
       " ('of', ((136, 1), 1.1565610001266253)),\n",
       " ('of', ((148, 1), 1.1565610001266253)),\n",
       " ('of', ((185, 1), 1.1565610001266253)),\n",
       " ('of', ((192, 1), 1.1565610001266253)),\n",
       " ('of', ((198, 1), 1.1565610001266253))]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join two RDD's with TF and with IDF by word\n",
    "\n",
    "rdd_merged = rdd_tf_flat.join(rdd_idf)\n",
    "rdd_merged.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 1.1565610001266253),\n",
       " (25, 1.1565610001266253),\n",
       " (33, 1.1565610001266253),\n",
       " (45, 1.1565610001266253),\n",
       " (49, 1.1565610001266253),\n",
       " (53, 1.1565610001266253),\n",
       " (73, 1.1565610001266253),\n",
       " (77, 1.1565610001266253),\n",
       " (93, 1.1565610001266253),\n",
       " (134, 1.1565610001266253),\n",
       " (136, 1.1565610001266253),\n",
       " (148, 1.1565610001266253),\n",
       " (185, 1.1565610001266253),\n",
       " (192, 1.1565610001266253),\n",
       " (198, 1.1565610001266253)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating tf-idf for each word in each document (word itself is not represented)\n",
    "\n",
    "def calculate_tf_idf(rdd):\n",
    "    return (rdd[1][0][0], rdd[1][0][1] * rdd[1][1]) \n",
    "\n",
    "rdd_merged_tfidf = rdd_merged.map(lambda row: calculate_tf_idf(row))\n",
    "rdd_merged_tfidf.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [7.162914159728358, 5.458166067489933]),\n",
       " (1, [5.458166067489933, 6.875232087276577]),\n",
       " (2, [5.458166067489933, 6.875232087276577]),\n",
       " (3, [7.008763479901099, 5.062853330845786]),\n",
       " (4, [7.162914159728358, 5.062853330845786]),\n",
       " (5, [5.062853330845786, 6.875232087276577]),\n",
       " (6, [6.875232087276577, 5.062853330845786]),\n",
       " (7, [7.568379267836522, 5.062853330845786]),\n",
       " (8, [7.856061340288304, 5.062853330845786]),\n",
       " (9, [7.856061340288304, 5.062853330845786]),\n",
       " (10,\n",
       "  [1.1565610001266253,\n",
       "   7.568379267836522,\n",
       "   2.7788063588506517,\n",
       "   2.861103858911277,\n",
       "   5.428313104340251,\n",
       "   7.856061340288304,\n",
       "   1.6183879689351153,\n",
       "   5.819179413027263,\n",
       "   5.622469118781209,\n",
       "   3.8670772937240288,\n",
       "   7.856061340288304]),\n",
       " (11,\n",
       "  [1.1565610001266253,\n",
       "   3.231088527004032,\n",
       "   1.0525560826799654,\n",
       "   7.568379267836522,\n",
       "   6.875232087276577,\n",
       "   1.9273591149166367,\n",
       "   3.065795670623532,\n",
       "   1.1903776225058949,\n",
       "   1.4066446590213988,\n",
       "   7.345235716522312,\n",
       "   7.345235716522312,\n",
       "   6.315616299341154,\n",
       "   1.7163214229096406,\n",
       "   2.075317824495974,\n",
       "   7.162914159728358]),\n",
       " (12,\n",
       "  [7.568379267836522,\n",
       "   1.0525560826799654,\n",
       "   5.819179413027263,\n",
       "   1.6183879689351153,\n",
       "   2.6742777899962182,\n",
       "   7.008763479901099,\n",
       "   7.162914159728358,\n",
       "   1.4066446590213988,\n",
       "   5.7766197986084675,\n",
       "   0.8581607114548203,\n",
       "   4.572646994282532,\n",
       "   2.77052473801893]),\n",
       " (13,\n",
       "  [1.1565610001266253,\n",
       "   3.231088527004032,\n",
       "   1.0525560826799654,\n",
       "   5.520686424471267,\n",
       "   4.572646994282532,\n",
       "   0.8581607114548203,\n",
       "   2.7281369596689475,\n",
       "   4.572646994282532,\n",
       "   5.622469118781209,\n",
       "   5.126032232467318,\n",
       "   6.556778356158042,\n",
       "   7.345235716522312]),\n",
       " (14,\n",
       "  [1.0525560826799654,\n",
       "   8.261526448396468,\n",
       "   4.142489273583996,\n",
       "   7.008763479901099,\n",
       "   5.9589413554024215,\n",
       "   7.008763479901099,\n",
       "   7.568379267836522,\n",
       "   7.856061340288304,\n",
       "   0.8581607114548203,\n",
       "   1.7761281451927131,\n",
       "   8.261526448396468])]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final grouping by doc_id to get RDD \"doc_id -> tf_idf vector\"\n",
    "\n",
    "rdd_merged_tfidf\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\\\n",
    "    .sortBy(lambda x: x[0])\\\n",
    "    .take(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
